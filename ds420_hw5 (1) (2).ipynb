{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>DS420: Homework 5 Spark Streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "In this homework, your goal to setup an end-to-end data streaming application to collect data from Twitter and analyze the data with Spark Streaming and Spark SQL. \n",
    "\n",
    "Firstly, we need to setup a Developer API acocunt with Twitter and create an application to get credentials. \n",
    "    \n",
    "Once you have that you then need to complete the `hw5_twitterKafkaProducer.py` program in order to setup the twitter stream producer. You can verify whether the producer is up running by starting a kafka-console-consumer on the side to monitor. \n",
    "\n",
    "Lastly, you need to finish this notebook to setup a Spark Streaming consumer, analyze the stream in real-time, and finally visualize some of the tweeter stream activities with Pandas. Keep your producer running while working on this notebook. \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Create a Spark session, and named it as `hw5_xxx`, where `xxx` is your last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 19:53:44 WARN Utils: Your hostname, GPUServer resolves to a loopback address: 127.0.1.1; using 10.4.10.8 instead (on interface enp2s0)\n",
      "22/05/09 19:53:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.0.1/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/moynihanl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/moynihanl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/opt/spark-3.0.1/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-390f72f1-0a89-461b-9153-de1c346c1c4d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.3-1 in central\n",
      "\tfound org.lz4#lz4-java;1.6.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.3 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.28 in central\n",
      ":: resolution report :: resolve 116ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.3-1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.6.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.28 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-390f72f1-0a89-461b-9153-de1c346c1c4d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/3ms)\n",
      "22/05/09 19:53:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/09 19:53:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Just change the appName but don't modify the configurations!\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkStreaming_PE6_moynihan')          \\\n",
    "                    .config('spark.jars.packages','org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1')\\\n",
    "                    .config('spark.jars.packages','org.apache.kafka:kafka-clients:2.4.1')\\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from kafka into a streaming DataFrame\n",
    "# Ensure to supply the bootstrap server and the channel;\n",
    "# and include the timestamp as you read it. \n",
    "\n",
    "streaming = spark.readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"ds420-moynihan\") \\\n",
    "  .option('includeTimestamp','True')\\\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only timestamp and value columns. Cast value column into String type.\n",
    "streaming = streaming.selectExpr( \"CAST(value AS STRING)\", 'timestamp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Run a simple query to show the raw data of the `value` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the `value` column only from the streaming DataFrame into rawValue\n",
    "rawValue = streaming.select('value')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 19:53:49 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a668137a-af81-4289-ab1f-230ac981ab56. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "# Start a streaming query with rawValue\n",
    "\n",
    "valueQuery = (rawValue.writeStream\n",
    "             .format('memory')\n",
    "             .queryName('valueQuery')\n",
    "             .start())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Poll the results every 5 seconds and show the first ten records\n",
    "# Run this for 5 iterations. \n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "for x in range(10):\n",
    "    df = spark.sql('SELECT * FROM valueQuery')\n",
    "    df.show(10)\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the query once it's done. \n",
    "\n",
    "valueQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Create schema for the raw data with the hint from the printed results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create the dataSchema with StructType()\n",
    "\n",
    "dataSchema = dataSchema = StructType([ \\\n",
    "    StructField(\"id\",StringType(),True), \\\n",
    "    StructField(\"user\",StringType(),True), \\\n",
    "    StructField(\"follower\",IntegerType(),True), \\\n",
    "    StructField(\"friends\", IntegerType(), True), \\\n",
    "    StructField(\"tweet\", StringType(), True), \\\n",
    "    StructField(\"hashtag\", IntegerType(), True), \\\n",
    "    StructField(\"ts\", TimestampType(),True), \\\n",
    "    StructField(\"retweet\",IntegerType(),True), \\\n",
    "    StructField(\"favorite\",IntegerType(),True) \\\n",
    " \n",
    "  ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(id,StringType,true),StructField(user,StringType,true),StructField(follower,IntegerType,true),StructField(friends,IntegerType,true),StructField(tweet,StringType,true),StructField(hashtag,IntegerType,true),StructField(ts,TimestampType,true),StructField(retweet,IntegerType,true),StructField(favorite,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "# Show the schema, use this as the hint to finish the question above\n",
    "\n",
    "print(dataSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new column for the `streaming` DataFrame named as `json`. Use `from_json` function to parse the string in `value` column into `json` column. Save the resulting DataFrame into `df`.\n",
    "`from_json` docs: [link](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.from_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = streaming.withColumn('json', from_json('value', dataSchema))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- json: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- user: string (nullable = true)\n",
      " |    |-- follower: integer (nullable = true)\n",
      " |    |-- friends: integer (nullable = true)\n",
      " |    |-- tweet: string (nullable = true)\n",
      " |    |-- hashtag: integer (nullable = true)\n",
      " |    |-- ts: timestamp (nullable = true)\n",
      " |    |-- retweet: integer (nullable = true)\n",
      " |    |-- favorite: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checkout the schema of df. \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Generate new columns with the sub-fields from the `json` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint1: once the dataSchema is contructed, we can use the `.name` and `.dataType` properties to find its field name and field data type, respectively. Run the code example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 id StringType\n",
      "1 user StringType\n",
      "2 follower IntegerType\n",
      "3 friends IntegerType\n",
      "4 tweet StringType\n",
      "5 hashtag IntegerType\n",
      "6 ts TimestampType\n",
      "7 retweet IntegerType\n",
      "8 favorite IntegerType\n"
     ]
    }
   ],
   "source": [
    "for idx, field in enumerate(dataSchema): \n",
    "    print(idx, field.name, field.dataType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint2: notice you can use dot operator to index into the nested structure. Run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[follower: int]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('json.follower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also works with bracket notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'json.follower AS `follower`'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['json.follower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, create new columns with all the sub-fields from the `json` column by using a for loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, field in enumerate(dataSchema): \n",
    "    a = df.withColumn(field.name, df['json.' + field.name])\n",
    "    df=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- json: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- user: string (nullable = true)\n",
      " |    |-- follower: integer (nullable = true)\n",
      " |    |-- friends: integer (nullable = true)\n",
      " |    |-- tweet: string (nullable = true)\n",
      " |    |-- hashtag: integer (nullable = true)\n",
      " |    |-- ts: timestamp (nullable = true)\n",
      " |    |-- retweet: integer (nullable = true)\n",
      " |    |-- favorite: integer (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- follower: integer (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- hashtag: integer (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- retweet: integer (nullable = true)\n",
      " |-- favorite: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checkout the schema after the update\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, clean the `df` DataFrame so that it only contains the sub-fileds from `json` column, and the `timestamp` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.select(['user','id', 'follower','tweet', 'hashtag', 'ts', 'retweet', 'favorite','timestamp', 'friends'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- follower: integer (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- hashtag: integer (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- retweet: integer (nullable = true)\n",
      " |-- favorite: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Run a query to find the occurences of each number of 'retweet'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweetCount =df.groupBy('retweet').count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 19:54:11 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a8b08088-2d5d-48ec-ab34-8473e19483a4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "retweetQuery = retweetCount.writeStream \\\n",
    "                  .outputMode('complete') \\\n",
    "                  .format('memory') \\\n",
    "                  .option('truncate', 'false') \\\n",
    "                  .queryName('retweetQuery') \\\n",
    "                  .start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:======================================>               (144 + 24) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:=================================================>    (182 + 18) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    df_ = spark.sql('SELECT * FROM retweetQuery')\n",
    "    df_.show(10)\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6: Run a windowed query to find the number of occurences for each number of 'follower'. Use window size as 20 seconds, and sliding frequency as 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "followerCount = df.groupBy(\n",
    "    window(df['timestamp'],'1 minutes','30 seconds'),\n",
    "    df['follower']\n",
    ").count().orderBy('window')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 19:54:31 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1badf097-2b47-42df-a28d-0c3b3c9398cc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "followerQuery = (followerCount.writeStream\n",
    "                  .outputMode('complete')\n",
    "                  .format('memory')\n",
    "                  .queryName('followerQuery')\n",
    "                  .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n",
      "+-------+-----+\n",
      "|retweet|count|\n",
      "+-------+-----+\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    df_ = spark.sql('SELECT * FROM retweetQuery')\n",
    "    df_.show(10)\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7: Analyze the sentiment of the tweets by using the TextBlob function.\n",
    "\n",
    "TextBlob help doc: [link](https://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, write a function that takes in a text input, and returns a sentiment of the input from the following array: ['negative', 'neutral', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    textblob = TextBlob(text).sentiment.polarity\n",
    "    if textblob < 0:\n",
    "        return 'negative'\n",
    "    elif textblob == 0:\n",
    "        return 'neutral'\n",
    "    elif textblob > 0:\n",
    "        return 'positive'\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, use a UDF function to generate a new column for the streaming DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "sentiment_udf = udf(get_sentiment, StringType())\n",
    "\n",
    "\n",
    "df = df.withColumn('sentiment', sentiment_udf(df.tweet))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- follower: integer (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- hashtag: integer (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- retweet: integer (nullable = true)\n",
      " |-- favorite: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lastly, run a query to find the count for each kind of sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentCount = df.groupBy('sentiment').count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 19:54:42 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-44c6a297-d5a1-4a5f-bd47-7a802514bbd7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "sentimentQuery = sentimentCount.writeStream \\\n",
    "                  .outputMode('complete') \\\n",
    "                  .format('memory') \\\n",
    "                  .queryName('sentimentQuery') \\\n",
    "                  .start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can also use an infinitely loop instead to poll the results\n",
    "\n",
    "for i in range(10):\n",
    "    df = spark.sql('SELECT * FROM sentimentQuery')\n",
    "    df.show(10)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8: Show the sentiment analysis on a barplot dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Only works for Jupyter Notebooks!\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28081/547683862.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_pd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lip/anaconda3/lib/python3.7/site-packages/seaborn/_decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lip/anaconda3/lib/python3.7/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[0;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   3183\u001b[0m                           \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3184\u001b[0m                           \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3185\u001b[0;31m                           errcolor, errwidth, capsize, dodge)\n\u001b[0m\u001b[1;32m   3186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lip/anaconda3/lib/python3.7/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, seed, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         self.establish_variables(x, y, hue, data, orient,\n\u001b[1;32m   1585\u001b[0m                                  order, hue_order, units)\n\u001b[0;32m-> 1586\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_statistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lip/anaconda3/lib/python3.7/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mestablish_colors\u001b[0;34m(self, color, palette, saturation)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# Determine the gray color to use for the lines framing the plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mlight_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolorsys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_to_hls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrgb_colors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mlum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlight_vals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m.6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb2hex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    df = spark.sql('SELECT * FROM sentimentQuery')\n",
    "    df_pd = df.toPandas()\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize = (10,8))\n",
    "    sns.barplot(data = df_pd, y='sentiment', x='count')\n",
    "    plt.show()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop all queries\n",
    "retweetQuery.stop()\n",
    "followerQuery.stop()\n",
    "sentimentQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great Job!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
