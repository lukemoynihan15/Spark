{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>DS420 - Lec17: Clustering Exercise (Part II: Data Analytics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "A famous politician who's running for an election needs your help - his laptop has been hacked! Luckily their forensic experts have grabbed valuable data about the hacks, including information like session time, locations, wpm typing speed, etc. The forensic experts relate to you what they have been able to figure out so far, they have been able to grab meta data of each session that the hackers used to connect to their servers. These are the features of the data:\n",
    "\n",
    "* 'Session_Connection_Time': How long the session lasted in minutes\n",
    "* 'Bytes Transferred': Number of MB transferred during session\n",
    "* 'Kali_Trace_Used': Indicates if the hacker was using Kali Linux\n",
    "* 'Servers_Corrupted': Number of server corrupted during the attack\n",
    "* 'Pages_Corrupted': Number of pages illegally accessed\n",
    "* 'Location': Location attack came from (Probably useless because the hackers used VPNs)\n",
    "* 'WPM_Typing_Speed': Their estimated typing speed based on session logs.\n",
    "\n",
    "\n",
    "There are 3 potential hackers that perpetrated the attack. The experts are certain of the first two hackers but they aren't very sure if the third hacker was involved or not. They have requested your help! Can you help figure out whether or not the third suspect had anything to do with the attacks, or was it just two hackers? It's probably not possible to know for sure, but maybe what you've just learned about Clustering can help!\n",
    "\n",
    "**One last key fact, the forensic experts know that the hackers trade off attacks. Meaning they should each have roughly the same amount of attacks. For example if there were 100 total attacks, then in a 2 hacker situation each should have about 50 hacks, in a three hacker situation each would have about 33 hacks. The engineer believes this is the key element to solving this, but doesn't know how to distinguish this unlabeled data into groups of hackers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup Environment and Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the spark path for Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark') # Path to my spark installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Spark Session with the following code. Modify the appName with your own name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1212, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:33607)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-f7e50c7997bf>\", line 4, in <module>\n",
      "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
      "  File \"/opt/spark/python/pyspark/sql/session.py\", line 186, in getOrCreate\n",
      "    sc = SparkContext.getOrCreate(sparkConf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 376, in getOrCreate\n",
      "    SparkContext(conf=conf or SparkConf())\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 136, in __init__\n",
      "    conf, jsc, profiler_cls)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 198, in _do_init\n",
      "    self._jsc = jsc or self._initialize_context(self._conf._jconf)\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 315, in _initialize_context\n",
      "    return self._jvm.JavaSparkContext(jconf)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1569, in __call__\n",
      "    answer, self._gateway_client, None, self._fqn)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lip/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f7e50c7997bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.mongodb.input.uri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mongodb://127.0.0.1/lec17.hacker\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.mongodb.output.uri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mongodb://127.0.0.1/lec17.hackerAnalysis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.jars.packages'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'org.mongodb.spark:mongo-spark-connector_2.12:3.0.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 136\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1569\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"kmeans_moynihan_p2\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/lec17.hacker\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/lec17.hackerAnalysis\")\\\n",
    "    .config('spark.jars.packages','org.mongodb.spark:mongo-spark-connector_2.12:3.0.0')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now load the data from the MongoDB: `lec17.hacker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6bafee2958cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read it back from MongoDB into a new Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.mongodb.spark.sql.DefaultSource\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uri\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mongodb://127.0.0.1/lec17.hacker\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Read it back from MongoDB into a new Dataframe\n",
    "df = spark.read\\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/lec17.hacker\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-eb589bae8d4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SparkSQL to find the users who can type faster than 74 words per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+--------------------+---------------+-----------------+-----------------------+----------------+--------------------+\n",
      "|Bytes Transferred|Kali_Trace_Used|            Location|Pages_Corrupted|Servers_Corrupted|Session_Connection_Time|WPM_Typing_Speed|                 _id|\n",
      "+-----------------+---------------+--------------------+---------------+-----------------+-----------------------+----------------+--------------------+\n",
      "|           364.88|              0|         Philippines|            7.0|             3.52|                   33.0|           74.24|[624105b0fc597801...|\n",
      "|            89.49|              1|Holy See (Vatican...|            8.0|              1.8|                   16.0|           74.92|[624105b0fc597801...|\n",
      "|           635.81|              0|            Slovenia|            7.0|             3.43|                   18.0|           74.01|[624105b0fc597801...|\n",
      "|           253.39|              0|Saint Vincent and...|            8.0|             2.66|                   19.0|            75.0|[624105b0fc597801...|\n",
      "|           517.46|              1|            Tanzania|            7.0|             3.24|                   12.0|           74.17|[624105b0fc597801...|\n",
      "+-----------------+---------------+--------------------+---------------+-----------------+-----------------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql('SELECT * FROM users where WPM_Typing_Speed > 74')\n",
    "\n",
    "\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export your query results to a new database on MongoDB: `lec17.hackerAnalysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write insight into MongoDB\n",
    "sqlDF.write\\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/lec17.hackerAnalysis\")\\\n",
    "    .mode('append')\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the basic statistics to all columns and justify why you need to standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------+------------------+-----------------+-----------------------+------------------+\n",
      "|summary| Bytes Transferred|   Kali_Trace_Used|   Location|   Pages_Corrupted|Servers_Corrupted|Session_Connection_Time|  WPM_Typing_Speed|\n",
      "+-------+------------------+------------------+-----------+------------------+-----------------+-----------------------+------------------+\n",
      "|  count|               334|               334|        334|               334|              334|                    334|               334|\n",
      "|   mean| 607.2452694610777|0.5119760479041916|       null|10.838323353293413|5.258502994011977|     30.008982035928145|57.342395209580864|\n",
      "| stddev|286.33593163576757|0.5006065264451406|       null|  3.06352633036022| 2.30190693339697|     14.088200614636158| 13.41106336843464|\n",
      "|    min|              10.0|                 0|Afghanistan|               6.0|              1.0|                    1.0|              40.0|\n",
      "|    max|            1330.5|                 1|   Zimbabwe|              15.0|             10.0|                   60.0|              75.0|\n",
      "+-------+------------------+------------------+-----------+------------------+-----------------+-----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.describe().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all columns in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bytes Transferred',\n",
       " 'Kali_Trace_Used',\n",
       " 'Location',\n",
       " 'Pages_Corrupted',\n",
       " 'Servers_Corrupted',\n",
       " 'Session_Connection_Time',\n",
       " 'WPM_Typing_Speed',\n",
       " '_id']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = ['Bytes Transferred',\n",
    " 'Kali_Trace_Used',\n",
    " 'Pages_Corrupted',\n",
    " 'Servers_Corrupted',\n",
    " 'Session_Connection_Time',\n",
    " 'WPM_Typing_Speed']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol = 'features')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = vec_assembler.transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize the vector feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inoutCol='features',\n",
    "                        outputCol='scaledFeatures',\n",
    "                        withMean=True, withStd=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(final_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize each feature to have unit standard deviation and zero mean\n",
    "cluster_final_data = scalerModel.transform(final_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time to find out whether its 2 or 3 hackers using KMeans!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KMeans\n",
    "\n",
    "import pyspark.ml.clustering import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two separate models for k=3 and k=2\n",
    "\n",
    "kmeans3 = KMeans(featCol='scaledFeatures', k=3, seed=101)\n",
    "\n",
    "kmeans2 =  KMeans(featCol='scaledFeatures', k=2, seed=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the two models\n",
    "\n",
    "model_k3 = kmeans3.fit(cluster_final_data)\n",
    "\n",
    "model_k2 = kmeans2.fit(cluster_final_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an evaluator object for the clutering\n",
    "# Evaluate clustering with Silhouette score\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol='scaledFeatures')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained models to make predictions\n",
    "\n",
    "pred_k3 = model_k3.transform(cluster_final_data)     #this is the part where u could use your testing data\n",
    "\n",
    "pred_k2 = model_k2.transform(cluster_final_data)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the predictions with silhouette score\n",
    "\n",
    "silhouette_k3 = evaluator.evaluate(pred_k3)\n",
    "\n",
    "silhouette_k2 = evaluator.evaluate(pred_k2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With K=3\n",
      "Silhouette coefficient = 0.30412315937808737\n",
      "------------------------------------------------------------\n",
      "With K=2\n",
      "Silhouette coefficient = 0.6683623593283755\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "\n",
    "print(\"With K=3\")\n",
    "print(\"Silhouette coefficient = \" + str(silhouette_k3))\n",
    "print('--'*30)\n",
    "print(\"With K=2\")\n",
    "print(\"Silhouette coefficient = \" + str(silhouette_k2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see when k=2, the score is much higher. This means using two groups describes the intrinsic data pattern better.\n",
    "\n",
    "#### We could however continue the analysis by seeing the \"elbow curve\" on a larger range of k values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coeff = []\n",
    "for i in range(2:11):\n",
    "    kmeans = KMeans[featureCol='scaledFeatures', k=i, seed = 101]\n",
    "    model = kmeans.fit(cluster_final_data)\n",
    "    pred = model.transform(cluster_final_data)\n",
    "    coeff.append(evaluator.evaluate(pred))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Silhouette Score')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(2,11), coeff, color='blue', linestyle='dashed',\n",
    "         marker='o',markerfacecolor='red', markersize=10\n",
    "\n",
    "\n",
    "\n",
    ")\n",
    "plt.title('Silhouette Score vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Silhouette Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nothing definitive can be said with the above, but wait! The last key fact that the engineer mentioned was that the attacks should be evenly numbered between the hackers! Let's check with the transform and prediction columns that result from this! Congratulations if you made this connection, it was quite tricky given what we've covered!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_k3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         2|   79|\n",
      "|         0|   88|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_k3.groupby('prediction').count().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|  167|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_k2.groupby('prediction').count().show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bingo! It was 2 hackers, in fact, our clustering algorithm created two equally sized clusters with K=2, no way that is a coincidence!\n",
    "\n",
    "# Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
