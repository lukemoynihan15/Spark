{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>DS420: Lab 6 Spark Streaming</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "\n",
    "In this lab, you are hired by a financial corporate to conduct fraud detection by applying spark streaming with `File` source.\n",
    "\n",
    "## Data:\n",
    "\n",
    "The data you are going to used are located at the shared HDFS: `/ds420_shared/paysim`\n",
    "\n",
    "The data simulate mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company, who is the provider of the mobile financial service which is currently running in more than 14 countries all around the world.\n",
    "\n",
    "## Schema:\n",
    "\n",
    "This is a sample of 1 row with headers explanation:\n",
    "\n",
    "`1,PAYMENT,1060.31,C429214117,1089.0,28.69,M1591654462,0.0,0.0,0,0`\n",
    "\n",
    "step - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 168 (7 days simulation). \n",
    "\n",
    "#### Note: `Each step of data is saved onto a unique file on the shared HDFS.`\n",
    "\n",
    "type - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\n",
    "\n",
    "amount -\n",
    "amount of the transaction in local currency.\n",
    "\n",
    "nameOrig - customer who started the transaction\n",
    "\n",
    "oldbalanceOrg - initial balance before the transaction\n",
    "\n",
    "newbalanceOrig - new balance after the transaction\n",
    "\n",
    "nameDest - customer who is the recipient of the transaction\n",
    "\n",
    "oldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).\n",
    "\n",
    "newbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Create a Spark session, and named it as lab6_xxx, where xxx is your last name. Note that you won't need the Kafka dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/07 14:38:04 WARN Utils: Your hostname, GPUServer resolves to a loopback address: 127.0.1.1; using 10.4.10.8 instead (on interface enp2s0)\n",
      "22/05/07 14:38:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.0.1/jars/spark-unsafe_2.12-3.0.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/moynihanl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/moynihanl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/opt/spark-3.0.1/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6e7d8860-d3ad-40c1-98f2-dc7814a97008;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.3-1 in central\n",
      "\tfound org.lz4#lz4-java;1.6.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.3 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.28 in central\n",
      ":: resolution report :: resolve 132ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.3-1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.6.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.28 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6e7d8860-d3ad-40c1-98f2-dc7814a97008\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/6ms)\n",
      "22/05/07 14:38:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/07 14:38:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/05/07 14:38:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Just change the appName but don't modify the configurations!\n",
    "\n",
    "spark = SparkSession.builder.appName('sparkStreaming_PE6_moynihan')          \\\n",
    "                    .config('spark.jars.packages','org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1')\\\n",
    "                    .config('spark.jars.packages','org.apache.kafka:kafka-clients:2.4.1')\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all the data files on the shared HDFS folder using `hdfs dfs` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 168 items\r\n",
      "-rw-r--r--   1 lip ds420     174022 2020-11-05 20:41 /ds420_shared/paysim/part-00000-004e6d42-bd34-44a9-a18b-2203ab3ad05d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        683 2020-11-10 16:51 /ds420_shared/paysim/part-00000-0084ddfd-ec6a-423c-8c7a-ad65f92726c9-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     315517 2020-11-10 16:51 /ds420_shared/paysim/part-00000-0135f73b-0457-4327-ae64-b9caf04b0ebe-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2276175 2020-11-10 16:51 /ds420_shared/paysim/part-00000-013c9a5c-b352-4e64-b968-c5640a9e27f7-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     353752 2020-11-10 16:51 /ds420_shared/paysim/part-00000-0180a491-d70a-4194-a8ac-79b89af6c987-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2088278 2020-11-10 16:51 /ds420_shared/paysim/part-00000-01cbd1ec-6256-47a9-a708-0841fc253022-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        810 2020-11-10 16:51 /ds420_shared/paysim/part-00000-02d36254-12b0-4b22-b1a1-5dc9fed1f893-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      66847 2020-11-10 16:51 /ds420_shared/paysim/part-00000-02e7c1e8-fbf8-4fd1-985f-34269ce3a094-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      38189 2020-11-10 16:51 /ds420_shared/paysim/part-00000-03148222-7edf-4111-b414-be0a8e10071d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1120 2020-11-10 16:51 /ds420_shared/paysim/part-00000-03f0d39c-760a-4ad4-926b-b8dc45a0d1ec-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     406108 2020-11-10 16:51 /ds420_shared/paysim/part-00000-03fe6da3-625f-41ea-9900-d2568a08866c-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1113 2020-11-10 16:51 /ds420_shared/paysim/part-00000-040678d6-1d5b-42b9-b716-f0dd509d57a8-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2234931 2020-11-10 16:51 /ds420_shared/paysim/part-00000-0409b0ca-f400-44a4-827e-4096edb2060c-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        833 2020-11-10 16:51 /ds420_shared/paysim/part-00000-040d1099-6136-4956-a2c3-42bf3d2ce02b-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1378 2020-11-10 16:51 /ds420_shared/paysim/part-00000-04330443-9eb6-4d4f-a116-8e5056fb6e0b-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2374343 2020-11-10 16:51 /ds420_shared/paysim/part-00000-04cc01b5-e545-4d37-ad7e-07c1b4e21cb6-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        385 2020-11-10 16:51 /ds420_shared/paysim/part-00000-04cf5f46-bcb1-4c6b-bd07-f76d6a74703c-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1538 2020-11-10 16:51 /ds420_shared/paysim/part-00000-05ddc4e9-7ee4-4498-a11f-720e69582aca-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     359352 2020-11-10 16:51 /ds420_shared/paysim/part-00000-05e5d788-cba8-4299-b8b0-94fdaf9e047d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        538 2020-11-10 16:51 /ds420_shared/paysim/part-00000-061e51da-a9de-4ac7-8950-e4cbe8696df5-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     276827 2020-11-10 16:51 /ds420_shared/paysim/part-00000-063c83cf-683a-4d57-a09e-045e5c4d3e15-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1111 2020-11-10 16:51 /ds420_shared/paysim/part-00000-064a5af7-cc68-4419-bed3-a256ee12ca76-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1717929 2020-11-10 16:51 /ds420_shared/paysim/part-00000-06902286-8527-4fab-8493-26c2f906a262-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2128403 2020-11-10 16:51 /ds420_shared/paysim/part-00000-0710756f-2abf-41ab-850a-d12d7c9a45a6-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     226235 2020-11-10 16:51 /ds420_shared/paysim/part-00000-077c353d-e207-4a2f-bc07-ebefe47f0e23-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        822 2020-11-10 16:51 /ds420_shared/paysim/part-00000-07962876-3a40-4fdc-af4c-d41ff70fcc2b-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1714896 2020-11-10 16:51 /ds420_shared/paysim/part-00000-07b811fe-82d9-4250-84ca-5ce00e76e621-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2291304 2020-11-10 16:51 /ds420_shared/paysim/part-00000-080c1cbd-696b-41cb-a0bf-e0d3aba07c32-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     194862 2020-11-10 16:52 /ds420_shared/paysim/part-00000-0871fc7f-a8be-4634-be33-70dd19e20b33-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     224800 2020-11-10 16:52 /ds420_shared/paysim/part-00000-088d7555-c5a4-40ff-bf04-1d445209bd4c-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        971 2020-11-10 16:52 /ds420_shared/paysim/part-00000-08d8991d-603e-42f0-9af0-3129108845fe-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2184127 2020-11-10 16:52 /ds420_shared/paysim/part-00000-09151c26-0447-4107-bdc7-fca54e399bfa-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     290366 2020-11-10 16:52 /ds420_shared/paysim/part-00000-09acbdc7-8755-48cc-8be8-f3eca2bfd14b-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2182523 2020-11-10 16:52 /ds420_shared/paysim/part-00000-0bd369b8-b7ad-4567-8088-781c458183a0-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     194425 2020-11-10 16:52 /ds420_shared/paysim/part-00000-0c057c2a-0081-43c9-81fb-57b334d2f5fd-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        836 2020-11-10 16:52 /ds420_shared/paysim/part-00000-0c2b27a2-7f7d-49a1-8adf-33bfc8608bce-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        524 2020-11-10 16:52 /ds420_shared/paysim/part-00000-0c6cb5ec-a090-4186-800a-b7558b95e23a-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     711122 2020-11-10 16:52 /ds420_shared/paysim/part-00000-0c8cc989-76db-4005-b77b-0a06110910b6-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        698 2020-11-10 16:52 /ds420_shared/paysim/part-00000-0c9b5b4d-e9a2-4a4c-83b6-a5280a994d69-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1926152 2020-11-10 16:53 /ds420_shared/paysim/part-00000-0d1678dd-32e6-448c-b25e-9bc7e4df0322-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        819 2020-11-10 16:53 /ds420_shared/paysim/part-00000-0d92e535-cdf8-4b94-b7d8-4b78504e77ba-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    3443434 2020-11-10 16:53 /ds420_shared/paysim/part-00000-0d98cf9c-796a-46fd-8bb5-896987506fa7-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2728485 2020-11-10 16:53 /ds420_shared/paysim/part-00000-0da92c6e-2602-4ea1-bcdd-28de121ca0e4-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1128 2020-11-10 16:53 /ds420_shared/paysim/part-00000-0e0bde5f-e99f-4d6f-8fae-c34c1b48c5b9-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1497560 2020-11-10 16:53 /ds420_shared/paysim/part-00000-0e621d2c-06b0-4062-b786-a2807e51d7ac-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     347151 2020-11-10 16:53 /ds420_shared/paysim/part-00000-0f217271-59a1-4602-b1e6-4c42a3357a29-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        393 2020-11-10 16:53 /ds420_shared/paysim/part-00000-0fbc15c7-4da8-42a7-8b55-5b34056b5184-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      31554 2020-11-10 16:53 /ds420_shared/paysim/part-00000-0fedb9a6-80d5-4c3a-b6b1-124702dbd1f1-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1085360 2020-11-10 16:53 /ds420_shared/paysim/part-00000-1074c457-8700-423e-8bae-14d62252cc95-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        824 2020-11-10 16:53 /ds420_shared/paysim/part-00000-109e722b-9514-4c53-92b0-e151149f7012-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2455933 2020-11-10 16:53 /ds420_shared/paysim/part-00000-10f69b7a-fd15-4e36-ad6a-7523d6bef380-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     171344 2020-11-10 16:53 /ds420_shared/paysim/part-00000-110d25ed-62c0-4d66-b380-5fc7c32d7653-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        682 2020-11-10 16:53 /ds420_shared/paysim/part-00000-112def29-3d79-4bf4-b370-40438de7a990-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2964665 2020-11-10 16:53 /ds420_shared/paysim/part-00000-1168e8b5-7729-4f5c-8f15-ca9c7fc59aca-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        384 2020-11-10 16:53 /ds420_shared/paysim/part-00000-11746ebe-fe82-46ae-9c98-4ded6158b327-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2482299 2020-11-10 16:53 /ds420_shared/paysim/part-00000-11dead6f-2d73-4d78-95ab-7b733ff38f67-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      71984 2020-11-10 16:53 /ds420_shared/paysim/part-00000-1239eccd-a3c1-4c14-a37f-d92be07eda1f-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      24148 2020-11-10 16:53 /ds420_shared/paysim/part-00000-12df8bda-f2f0-452f-a1f9-2f74a6c4503d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    3241236 2020-11-10 16:53 /ds420_shared/paysim/part-00000-12f400ee-c435-4c01-bab9-51d1657c4f87-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        529 2020-11-10 16:53 /ds420_shared/paysim/part-00000-12f685b0-9f83-4e0c-a285-f83caf5fa804-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     563168 2020-11-10 19:35 /ds420_shared/paysim/part-00000-143219dc-419a-442d-8b41-f73c8b310f09-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1120 2020-11-10 19:35 /ds420_shared/paysim/part-00000-1495e786-7e01-49d9-b3fe-134129aa4ca0-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     578564 2020-11-10 19:35 /ds420_shared/paysim/part-00000-15126187-ec04-4fbd-87f7-dc98eb36aea4-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       2972 2020-11-10 19:35 /ds420_shared/paysim/part-00000-1568764a-e129-4389-9493-60e7d6cfe667-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     399640 2020-11-10 19:35 /ds420_shared/paysim/part-00000-15c7edfe-aaf1-4f68-8b5d-3ee35ad9495d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1691 2020-11-10 19:35 /ds420_shared/paysim/part-00000-165e925e-ee4b-4fde-bf26-b1eae814d12b-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     130393 2020-11-10 19:35 /ds420_shared/paysim/part-00000-167992c0-0de4-4b68-9111-e30766e56d8b-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     191163 2020-11-10 19:35 /ds420_shared/paysim/part-00000-16c8eb30-573a-4879-9ce5-40d442d0a551-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1086 2020-11-10 19:35 /ds420_shared/paysim/part-00000-17474b51-5e6e-48d8-83a3-e7707aafeafa-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1087 2020-11-10 19:35 /ds420_shared/paysim/part-00000-177277f1-6767-492d-a33e-ae95c6c4e39d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     482427 2020-11-10 19:35 /ds420_shared/paysim/part-00000-1891aaad-125e-4f85-94a7-80625acc17e7-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        967 2020-11-10 19:35 /ds420_shared/paysim/part-00000-18eb61e9-a54e-4dfe-974f-05c8355d0a0d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        697 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1942cfa5-2af9-4cd8-b799-b32a335f7830-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        815 2020-11-10 19:36 /ds420_shared/paysim/part-00000-19437d9e-2027-4a89-a4c1-5e29b30abf99-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1098 2020-11-10 19:36 /ds420_shared/paysim/part-00000-195d450a-6b20-450e-8189-6a7f739daa53-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        535 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1983b2f8-82aa-47bc-8c08-21c3c5fc61d6-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2203333 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1991d533-399d-4337-a03f-2fcb5d868ba4-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        670 2020-11-10 19:36 /ds420_shared/paysim/part-00000-19c7e0bf-6905-4b75-8613-af923851edc4-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     380297 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1a8f155d-ed83-449b-bae8-fbf217b2b388-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1571 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1bc05b02-7732-476a-a6ff-ffd7e9e4b8b5-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        835 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1c4d5da9-85ac-4df1-9c60-b1eec939f00a-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        672 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1c5d532e-b8ce-4df9-9cc8-36d865c69db1-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        975 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1c5f678b-0198-4790-896e-09cff786e7bf-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1993136 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1ca8329b-a603-46c6-9492-1f74adb1e36f-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2716153 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1cb613f0-c781-43df-8f7e-e32a3f756569-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        523 2020-11-10 19:36 /ds420_shared/paysim/part-00000-1d0d3edc-a193-42a0-946c-a083556c45f8-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      33065 2020-11-10 19:37 /ds420_shared/paysim/part-00000-1d7ef022-6cb7-4c14-b715-ab02724032a6-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1554 2020-11-10 19:37 /ds420_shared/paysim/part-00000-1e1c6e42-a543-44a0-b08d-9d1bb0da8205-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1208129 2020-11-10 19:37 /ds420_shared/paysim/part-00000-1e945603-5d41-48ba-96bc-dcc49993ded9-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       3306 2020-11-10 19:37 /ds420_shared/paysim/part-00000-1ea4d5f4-c064-47b8-b44b-9d61a1dc0e0c-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     472274 2020-11-10 19:37 /ds420_shared/paysim/part-00000-1eb4d103-8de6-4ee0-9347-cde3bc39b3df-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        989 2020-11-10 19:37 /ds420_shared/paysim/part-00000-1f1148f6-bc56-4cc3-8fb5-898fde854e63-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2249286 2020-11-10 19:37 /ds420_shared/paysim/part-00000-1f55520e-38a0-435a-b824-be366c388727-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2176437 2020-11-10 19:37 /ds420_shared/paysim/part-00000-1f6882ad-b772-4dfc-9794-e1e3d1d224bb-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        695 2020-11-10 19:37 /ds420_shared/paysim/part-00000-1fbaa33b-71cc-4616-8c8a-7996a4a028e6-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1276 2020-11-10 19:37 /ds420_shared/paysim/part-00000-2040f018-fb00-402e-be91-c4350383d770-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        385 2020-11-10 19:37 /ds420_shared/paysim/part-00000-20c496a7-0b32-45f6-9747-d76551740ebe-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     573428 2020-11-10 19:37 /ds420_shared/paysim/part-00000-21140f15-d6d7-4461-a3ba-82f4064bbf75-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     359984 2020-11-10 19:37 /ds420_shared/paysim/part-00000-2126cff3-1bd1-4f2f-bf20-a0e8351a7004-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        841 2020-11-10 19:37 /ds420_shared/paysim/part-00000-212d119e-2835-4011-947c-16f9782ffb7f-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      45894 2020-11-10 19:37 /ds420_shared/paysim/part-00000-21738f4c-df23-4a65-bb78-eb5467164cc5-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        833 2020-11-10 19:37 /ds420_shared/paysim/part-00000-21851786-67ea-42b3-affd-c71fd14935a8-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2293775 2020-11-10 19:37 /ds420_shared/paysim/part-00000-21c836b8-6e00-403e-aba3-8856ad8c08b2-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1060729 2020-11-10 19:37 /ds420_shared/paysim/part-00000-21ce58c9-b115-446a-a9a1-102d874a7b22-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      38972 2020-11-10 19:37 /ds420_shared/paysim/part-00000-21d8d825-e7fa-4e75-b12f-fdaf5f096f30-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     206950 2020-11-10 19:37 /ds420_shared/paysim/part-00000-21f24709-dd23-44f2-8d10-f89f2e32ed47-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2448667 2020-11-10 19:37 /ds420_shared/paysim/part-00000-2262a7b2-e1bf-4b12-b69f-7132465c2256-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      32678 2020-11-10 19:37 /ds420_shared/paysim/part-00000-23044671-e3d3-47bc-8675-32da1aec4c0a-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1875 2020-11-10 19:37 /ds420_shared/paysim/part-00000-23345c76-23ef-4453-bea6-ac1b5aa2816d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        837 2020-11-10 19:37 /ds420_shared/paysim/part-00000-2338f2e0-2d37-4e90-a88c-742cc0b64a8e-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        374 2020-11-10 19:37 /ds420_shared/paysim/part-00000-237b93f1-1e22-4354-869b-2f775d9f38e6-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     189241 2020-11-10 19:37 /ds420_shared/paysim/part-00000-23ba1e2e-b11c-41f6-8a45-fbcc1bc21691-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2239374 2020-11-10 19:37 /ds420_shared/paysim/part-00000-23ecb6f9-635d-4ec0-85de-cd5d32639c70-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1995968 2020-11-10 19:37 /ds420_shared/paysim/part-00000-25231ce8-5eb4-4f80-825e-564bffbcf028-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     429835 2020-11-10 19:37 /ds420_shared/paysim/part-00000-25534787-3983-452d-8a3a-7ada616b39bb-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        978 2020-11-10 19:37 /ds420_shared/paysim/part-00000-257b7f44-d2dd-4f6a-9647-d98bbd176b1b-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      38756 2020-11-10 19:37 /ds420_shared/paysim/part-00000-25fce102-94fd-48c0-beb5-5c6f37a77e24-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2255050 2020-11-10 19:37 /ds420_shared/paysim/part-00000-262a6758-3567-4124-9790-8757ca00dd43-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     871869 2020-11-10 19:37 /ds420_shared/paysim/part-00000-2699522f-cc1a-4d18-877f-45187b187311-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     176216 2020-11-10 19:37 /ds420_shared/paysim/part-00000-26e1dd25-e163-441a-9453-d393e093a3b3-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        961 2020-11-10 19:37 /ds420_shared/paysim/part-00000-2736caa5-8d0d-4348-a41a-a71dc72027f9-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        966 2020-11-10 19:37 /ds420_shared/paysim/part-00000-280f7958-3b1a-455a-9957-4c5839d3db8e-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1054297 2020-11-10 19:37 /ds420_shared/paysim/part-00000-28119820-4f69-4622-83d8-5502d002728b-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      95025 2020-11-10 19:38 /ds420_shared/paysim/part-00000-282363e9-09fc-4f76-8ac9-14d933059f30-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1136 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2848e1fc-6747-4355-b301-fc71d15fa34d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2052356 2020-11-10 19:38 /ds420_shared/paysim/part-00000-28bf9af4-bc5b-4804-aabc-9eb2fd4085b9-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        540 2020-11-10 19:38 /ds420_shared/paysim/part-00000-296665ac-30d5-4be0-b9d2-4b6f5190d2f7-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1424 2020-11-10 19:38 /ds420_shared/paysim/part-00000-296b5188-80c2-4f85-90d8-9c0ac5bfa39a-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1515 2020-11-10 19:38 /ds420_shared/paysim/part-00000-29dd6503-c412-4624-a484-9aa70586cdbc-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    3029522 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2ac056d4-80b5-46cb-9d2c-2f200dcf4625-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        682 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2b894d5a-b372-4aaf-9552-6f43bb37cec8-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1269 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2ca62011-b77b-429e-99c6-c1f02848d28e-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     104581 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2d1ec8f1-cd90-42cf-b17e-62fd9a8df3f4-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     264677 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2d6fed1b-0c8d-4fe8-aa1b-ee64d3cfb00d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     458615 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2d7a612e-ec49-4137-8c68-a1930d784cff-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        992 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2de29f88-bb92-4c00-bbb3-4b9371dbeb19-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      83104 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2e23679e-198a-49f7-8077-c6da20248a79-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2601700 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2e23c6e2-37e7-452b-83c0-8b8c3eb8137b-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        827 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2e497141-f05f-4909-817c-5b4f6950bb03-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        814 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2e7bd2b6-f874-47aa-9b38-b6e98e2ba139-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1625215 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2f0f8399-0bc1-4015-b86e-fc5646832ceb-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      48939 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2f8689d4-06fa-466c-b760-11b835a84ec0-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1380189 2020-11-10 19:38 /ds420_shared/paysim/part-00000-2f8afe5d-5b10-4621-8554-5a438a0393f7-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        532 2020-11-10 19:38 /ds420_shared/paysim/part-00000-3026feee-25dc-422f-8307-6e134404fbc6-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     294759 2020-11-10 19:38 /ds420_shared/paysim/part-00000-322d9d49-9a92-42c2-9cd7-63dcb033786d-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     101273 2020-11-10 19:38 /ds420_shared/paysim/part-00000-3256e29d-11d9-40e7-b2ae-756d6d81f8fc-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1397 2020-11-10 19:38 /ds420_shared/paysim/part-00000-32e2562f-fe1b-4d13-ad45-44e26765bf6e-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      23495 2020-11-10 19:38 /ds420_shared/paysim/part-00000-3302ffae-3ed8-4f3b-aa15-504475d56375-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1212214 2020-11-10 19:38 /ds420_shared/paysim/part-00000-3375fd24-b387-4f09-93c3-a52ac0269c13-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     824124 2020-11-10 19:38 /ds420_shared/paysim/part-00000-3377462c-653e-40e6-95e3-45ecb4f66a9c-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        835 2020-11-10 19:38 /ds420_shared/paysim/part-00000-33f24f11-6c93-4aac-8636-56653db76776-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    3010436 2020-11-10 19:38 /ds420_shared/paysim/part-00000-34bf54c5-1e67-44d8-93f0-c96272c06471-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     198911 2020-11-10 19:38 /ds420_shared/paysim/part-00000-34d94e9c-ede4-4c5a-98f2-33513db86169-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2659135 2020-11-10 19:38 /ds420_shared/paysim/part-00000-354296f0-9c00-4424-b1bc-c57901e3df2a-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    3313989 2020-11-10 19:38 /ds420_shared/paysim/part-00000-35b090b9-802b-48e3-aad8-27aaaf744cc5-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      54555 2020-11-10 19:38 /ds420_shared/paysim/part-00000-3602bee1-03d3-4a21-901a-f55619e183b1-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2086649 2020-11-10 19:38 /ds420_shared/paysim/part-00000-3605cc25-9c4e-4aa1-8ad1-8d8a9e0197f4-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1255 2020-11-10 19:38 /ds420_shared/paysim/part-00000-363428c7-91a6-4b12-8013-861ea1031c59-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420      64316 2020-11-10 19:38 /ds420_shared/paysim/part-00000-374b6883-80d2-4c2c-8c23-7d3edcc9dcb4-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       5687 2020-11-10 19:38 /ds420_shared/paysim/part-00000-38469a91-9b37-4777-80e6-87e2926a4e18-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     549270 2020-11-10 19:39 /ds420_shared/paysim/part-00000-387dfb40-ead6-4cc3-a7aa-bf2f8e851826-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    1917730 2020-11-10 19:39 /ds420_shared/paysim/part-00000-38f4fed7-7a29-4d4d-9456-7ce6d7766c63-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        680 2020-11-10 19:39 /ds420_shared/paysim/part-00000-39149521-583d-4353-b2a5-9d099481c656-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    2502147 2020-11-10 19:39 /ds420_shared/paysim/part-00000-39b83ce8-4d50-470a-9a35-1ad4704fca6e-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420     866609 2020-11-10 19:39 /ds420_shared/paysim/part-00000-3a2450cf-abc2-4523-8b0e-624a6670ac41-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420    3339644 2020-11-10 19:39 /ds420_shared/paysim/part-00000-3a5134bd-7b11-4ab9-b87b-12b95d54f713-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420        981 2020-11-10 19:39 /ds420_shared/paysim/part-00000-3a6d84b5-86a6-4f98-8ab8-588f403cccb6-c000.csv\r\n",
      "-rw-r--r--   1 lip ds420       1090 2020-11-10 19:39 /ds420_shared/paysim/part-00000-3ad3d8ab-4c09-46db-aff1-51fd705a14f1-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!/home/hadoop/hadoop-3.2.1/bin/hdfs dfs -ls /ds420_shared/paysim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first try to read in ONLY the first file on the list as a non-streaming DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleFile = spark.read.csv('hdfs://localhost:9000/ds420_shared/paysim/part-00000-004e6d42-bd34-44a9-a18b-2203ab3ad05d-c000.csv',inferSchema = True, \n",
    "                      header = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Study the `singleFile` DataFrame.\n",
    "\n",
    "#### Show the top 10 rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "|step|    type|   amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "| 429|CASH_OUT|102499.12|C1849492127|      11034.0|           0.0|C1250506130|      79834.67|     182333.79|\n",
      "| 429| PAYMENT|  1780.49| C587127232|      30522.0|      28741.51| M627292834|           0.0|           0.0|\n",
      "| 429| PAYMENT|  1722.22|C2140027529|       5592.0|       3869.78|M2040657437|           0.0|           0.0|\n",
      "| 429| PAYMENT|  3828.73| C759496752|      39899.0|      36070.27| M365631888|           0.0|           0.0|\n",
      "| 429|TRANSFER| 63129.48| C455029714|       4048.0|           0.0|C1828878126|           0.0|      63129.48|\n",
      "| 429| CASH_IN|126025.28|C1844981682|      20968.0|     146993.28| C844561002|     217592.62|      91567.33|\n",
      "| 429| CASH_IN| 62506.97|C1757013693|      21155.0|      83661.97|C1900575049|     943658.47|      881151.5|\n",
      "| 429| CASH_IN|141146.24| C636712355|     770188.0|     911334.24| C779031448|    2646146.89|    2505000.65|\n",
      "| 429|TRANSFER|134780.38|C1192252268|     164488.0|      29707.62| C201805373|           0.0|     134780.38|\n",
      "| 429| PAYMENT|  3709.91|C1798922627|      18772.0|      15062.09|M1276313175|           0.0|           0.0|\n",
      "+----+--------+---------+-----------+-------------+--------------+-----------+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "singleFile.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the schema of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "singleFile.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify whether this file only contains a single `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|step|count|\n",
      "+----+-----+\n",
      "| 429| 2357|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "singleFile.groupBy('step').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To cheat it a little bit, let's save the schema of `singleFile` for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(step,IntegerType,true),StructField(type,StringType,true),StructField(amount,DoubleType,true),StructField(nameOrig,StringType,true),StructField(oldbalanceOrg,DoubleType,true),StructField(newbalanceOrig,DoubleType,true),StructField(nameDest,StringType,true),StructField(oldbalanceDest,DoubleType,true),StructField(newbalanceDest,DoubleType,true)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSchema = singleFile.schema\n",
    "\n",
    "\n",
    "dataSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Letâ€™s create a streaming version of this input, we'll read each file one by one as if it was a stream. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: option `'maxFilesPerTrigger'` allows you to control how quickly Spark will read all of the files in the folder. In this example we're limiting the flow of the stream to one file per trigger. \n",
    "+ Ensure to provide the data schema as you read in the data stream. \n",
    "+ Ensure to add in a new `'timestamp'` column while reading the data in using `.withColumn()`. Consider using pyspark build-in function [current_timestamp()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=date#pyspark.sql.functions.current_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = (spark.readStream.\\\n",
    "             schema(dataSchema)\\\n",
    "             .option('maxFilesPerTrigger', 1)\\\n",
    "             .option('includeTimeStamp','true')\\\n",
    "             .csv(\"hdfs://localhost:9000/ds420_shared/paysim\")\\\n",
    "             .withColumn('timestamp', F.current_timestamp())\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify if the DataFrame is streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the schema, note there is a 'timestamp' column by the end. Very nice! Looks like the data retains the correct schema. This will save us lots of time on data parsing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Let's set up a streaming query.\n",
    "\n",
    "#### Assume that if there are many transcations ended at the same destination, then it's likely to be a fraud. Knowing the `nameDest` column is the recipient ID of the transaction, let's write a transformation to sort the `nameDest` in descending order with `nameDest`'s number of occurences. \n",
    "\n",
    "Note: in the last lecture, we discussed that `orderBy` cannot be easily achieved when using Kafka sources. However, we should be able to use it with file sources, as they are deterministic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_count = streaming.groupBy('nameDest').count().orderBy('count',ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have our transformation, we need to specify an output sink for the results.\n",
    "\n",
    "+ For this example, we're going to write to a memory sink which keeps the results in memory.\n",
    "\n",
    "+ We also need to define how Spark will output that data. In this example, we'll use the complete output mode (rewriting all of the keys along with their counts after every trigger).\n",
    "\n",
    "+ In this example we won't include activityQuery.awaitTermination() because it is required only to prevent the driver process from terminating when the stream is active. So in order to be able to run this locally in a notebook we won't include it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/27 13:06:57 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-010e34f1-0240-413b-b3c6-21f79b8aec0c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "activityQuery = (\n",
    "\n",
    "    dest_count.writeStream\n",
    "         .outputMode('complete')\n",
    "         .format('memory')\n",
    "         .option('transaction','false')\n",
    "         .queryName('activityQuery')\n",
    "         .start()\n",
    " \n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activityQuery.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now use a loop to poll the query results from last cell. Generate 20 interations, and pause for 0.5 second for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|nameDest|count|\n",
      "+--------+-----+\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:==============>                                       (52 + 24) / 200]\r",
      "\r",
      "[Stage 14:======================>                               (85 + 24) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|nameDest|count|\n",
      "+--------+-----+\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|nameDest|count|\n",
      "+--------+-----+\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:=================================>                   (126 + 24) / 200]\r",
      "\r",
      "[Stage 16:=============================================>       (172 + 24) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|nameDest|count|\n",
      "+--------+-----+\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1096358251|    2|\n",
      "|M1629154613|    1|\n",
      "| M113229583|    1|\n",
      "| C530298945|    1|\n",
      "| C473286442|    1|\n",
      "|M1136484117|    1|\n",
      "| C207699093|    1|\n",
      "| C556845837|    1|\n",
      "| C115890198|    1|\n",
      "|M1520465793|    1|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:==========================================>          (159 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1096358251|    2|\n",
      "|M1629154613|    1|\n",
      "| M113229583|    1|\n",
      "| C530298945|    1|\n",
      "| C473286442|    1|\n",
      "|M1136484117|    1|\n",
      "| C207699093|    1|\n",
      "| C556845837|    1|\n",
      "| C115890198|    1|\n",
      "|M1520465793|    1|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1096358251|    2|\n",
      "|   nameDest|    2|\n",
      "| C473286442|    1|\n",
      "|M1136484117|    1|\n",
      "|C1469065727|    1|\n",
      "|M1629154613|    1|\n",
      "| M113229583|    1|\n",
      "| C530298945|    1|\n",
      "| C207699093|    1|\n",
      "|C1353252300|    1|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:=================================>                   (128 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1096358251|    2|\n",
      "|   nameDest|    2|\n",
      "| C473286442|    1|\n",
      "|M1136484117|    1|\n",
      "|C1469065727|    1|\n",
      "|M1629154613|    1|\n",
      "| M113229583|    1|\n",
      "| C530298945|    1|\n",
      "| C207699093|    1|\n",
      "|C1353252300|    1|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 33:=============================================>       (173 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    3|\n",
      "|C1096358251|    2|\n",
      "| C917200245|    2|\n",
      "| C129866793|    2|\n",
      "| C203170356|    2|\n",
      "| C256667415|    2|\n",
      "| C695944516|    2|\n",
      "|C1390295291|    2|\n",
      "|C1950148148|    2|\n",
      "|C1791332027|    2|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    3|\n",
      "|C1096358251|    2|\n",
      "| C917200245|    2|\n",
      "| C129866793|    2|\n",
      "| C203170356|    2|\n",
      "| C256667415|    2|\n",
      "| C695944516|    2|\n",
      "|C1390295291|    2|\n",
      "|C1950148148|    2|\n",
      "|C1791332027|    2|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 40:==============================================>      (174 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    4|\n",
      "|C1646402381|    3|\n",
      "| C669865371|    3|\n",
      "| C967163457|    3|\n",
      "|C1469679082|    3|\n",
      "|C2047987632|    3|\n",
      "|C1711718999|    3|\n",
      "| C523421579|    3|\n",
      "|C2092899449|    3|\n",
      "| C415050880|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 43:=====================================>               (140 + 25) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    4|\n",
      "|C1646402381|    3|\n",
      "| C669865371|    3|\n",
      "| C967163457|    3|\n",
      "|C1469679082|    3|\n",
      "|C2047987632|    3|\n",
      "|C1711718999|    3|\n",
      "| C523421579|    3|\n",
      "|C2092899449|    3|\n",
      "| C415050880|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    5|\n",
      "| C447348916|    4|\n",
      "|C1646402381|    3|\n",
      "| C669865371|    3|\n",
      "| C967163457|    3|\n",
      "|C1469679082|    3|\n",
      "|C2047987632|    3|\n",
      "|C1430052429|    3|\n",
      "|C1711718999|    3|\n",
      "| C523421579|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 50:=====================================>               (140 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    5|\n",
      "| C447348916|    4|\n",
      "|C1646402381|    3|\n",
      "| C669865371|    3|\n",
      "| C967163457|    3|\n",
      "|C1469679082|    3|\n",
      "|C2047987632|    3|\n",
      "|C1430052429|    3|\n",
      "|C1711718999|    3|\n",
      "| C523421579|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    6|\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "|C1540448538|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:========================================>            (151 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    6|\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "|C1540448538|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 61:===================================>                 (134 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    7|\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "|C1540448538|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 64:===========================================>         (166 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    7|\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "|C1540448538|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 68:==================================>                  (129 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    8|\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "|C1540448538|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 71:============================================>        (167 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|   nameDest|    8|\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "|C1540448538|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for x in range(20):\n",
    "    df = spark.sql('SELECT * FROM activityQuery')\n",
    "\n",
    "    df.show(10)\n",
    "    time.sleep(.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: Handling an active streaming query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the results from the last cell contains a row named `nameDest`. These are missing data entries from the original dataset. Let's re-write the SQL query in the loop to exclude `nameDest` and only show the recipient IDs which appear at least `twice`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 75:======================================>              (144 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "|C1540448538|    3|\n",
      "|C1639805485|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 78:=============================================>       (170 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "|C1540448538|    3|\n",
      "|C1639805485|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 82:========================================>            (151 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "|C1540448538|    3|\n",
      "|C1639805485|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 85:===================================================>  (192 + 8) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "|C1540448538|    3|\n",
      "|C1639805485|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 89:==========================================>          (161 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1217358061|    4|\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "|C1668505392|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1217358061|    4|\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "|C1668505392|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 96:=============================================>       (173 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1217358061|    4|\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "|C1668505392|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C1217358061|    4|\n",
      "|C1662173477|    4|\n",
      "|C2089017191|    4|\n",
      "| C447348916|    4|\n",
      "|C1668505392|    4|\n",
      "| C128187691|    4|\n",
      "|C1646402381|    3|\n",
      "|C1616817673|    3|\n",
      "|C1175586823|    3|\n",
      "| C669865371|    3|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 103:============================================>       (172 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "|C2047987632|    4|\n",
      "|C1711718999|    4|\n",
      "| C189021232|    4|\n",
      "|C1662173477|    4|\n",
      "| C415050880|    4|\n",
      "|C2089017191|    4|\n",
      "|C2033049104|    4|\n",
      "| C329387723|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "|C2047987632|    4|\n",
      "|C1711718999|    4|\n",
      "| C189021232|    4|\n",
      "|C1662173477|    4|\n",
      "| C415050880|    4|\n",
      "|C2089017191|    4|\n",
      "|C2033049104|    4|\n",
      "| C329387723|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 110:=======================================>            (151 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "|C2047987632|    4|\n",
      "|C1711718999|    4|\n",
      "| C189021232|    4|\n",
      "|C1662173477|    4|\n",
      "| C415050880|    4|\n",
      "|C2089017191|    4|\n",
      "|C2033049104|    4|\n",
      "| C329387723|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 113:==============================================>     (179 + 21) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "|C2047987632|    4|\n",
      "|C1711718999|    4|\n",
      "| C189021232|    4|\n",
      "|C1662173477|    4|\n",
      "| C415050880|    4|\n",
      "|C2089017191|    4|\n",
      "|C2033049104|    4|\n",
      "| C329387723|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 117:================================>                   (125 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "|C2047987632|    4|\n",
      "|C1711718999|    4|\n",
      "| C189021232|    4|\n",
      "|C1662173477|    4|\n",
      "| C415050880|    4|\n",
      "|C2089017191|    4|\n",
      "|C2033049104|    4|\n",
      "| C329387723|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "|C2047987632|    4|\n",
      "|C1711718999|    4|\n",
      "| C189021232|    4|\n",
      "|C1662173477|    4|\n",
      "| C415050880|    4|\n",
      "|C2089017191|    4|\n",
      "|C2033049104|    4|\n",
      "| C329387723|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2033049104|    5|\n",
      "| C944900274|    4|\n",
      "|C1620683242|    4|\n",
      "| C810358289|    4|\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "| C519261901|    4|\n",
      "|C2047987632|    4|\n",
      "|C1664952213|    4|\n",
      "|C1217280492|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 130:=========================================>          (161 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2033049104|    5|\n",
      "| C944900274|    4|\n",
      "|C1620683242|    4|\n",
      "| C810358289|    4|\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "| C519261901|    4|\n",
      "|C2047987632|    4|\n",
      "|C1664952213|    4|\n",
      "|C1217280492|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2033049104|    5|\n",
      "| C944900274|    4|\n",
      "|C1620683242|    4|\n",
      "| C810358289|    4|\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "| C519261901|    4|\n",
      "|C2047987632|    4|\n",
      "|C1664952213|    4|\n",
      "|C1217280492|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2033049104|    5|\n",
      "| C944900274|    4|\n",
      "|C1620683242|    4|\n",
      "| C810358289|    4|\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "| C519261901|    4|\n",
      "|C2047987632|    4|\n",
      "|C1664952213|    4|\n",
      "|C1217280492|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 143:=============================================>      (176 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2033049104|    5|\n",
      "| C944900274|    4|\n",
      "|C1620683242|    4|\n",
      "| C810358289|    4|\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "| C519261901|    4|\n",
      "|C2047987632|    4|\n",
      "|C1664952213|    4|\n",
      "|C1217280492|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|   nameDest|count|\n",
      "+-----------+-----+\n",
      "|C2033049104|    5|\n",
      "| C944900274|    4|\n",
      "|C1620683242|    4|\n",
      "| C810358289|    4|\n",
      "|C2107268831|    4|\n",
      "|C1217358061|    4|\n",
      "| C519261901|    4|\n",
      "|C2047987632|    4|\n",
      "|C1664952213|    4|\n",
      "|C1217280492|    4|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for x in range(20):\n",
    "    df = spark.sql('SELECT * FROM activityQuery WHERE NOT nameDest = \"nameDest\" AND count>2')\n",
    "\n",
    "    df.show(10)\n",
    "    time.sleep(.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if stream is still active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 162:============================================>       (172 + 24) / 200]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 167:======================================>             (149 + 24) / 200]\r"
     ]
    }
   ],
   "source": [
    "streaming.isStreaming\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the status of the `activityQuery`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Processing new data',\n",
       " 'isDataAvailable': True,\n",
       " 'isTriggerActive': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "activityQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's turn off the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6: Write a windowed query to find the total transcation dollar amount for each payment type.\n",
    "\n",
    "+ Show only the aggregated `'amount'` and rename it to `'TotalAmount'`. (Consider using `withColumnRenamed`).\n",
    "+ Use window interval as 20 seconds, and sliding interval as 10 seconds.\n",
    "+ Ensure to order the result by `'window'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the windowed transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalAmount = (streaming.groupBy('type', window('timestamp', '20 seconds', '10 seconds'))\n",
    "               .sum('amount')\n",
    "               .withColumnRenamed('sum(amount)', 'TotalAmount')\n",
    "               .orderBy('window')\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totalAmount.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- type: string (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totalAmount.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register a query and specify the output sink as memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/27 13:07:57 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "winQuery = ( \n",
    "    \n",
    "\n",
    "    totalAmount.writeStream\n",
    "         .outputMode('complete')\n",
    "         .format('memory')\n",
    "         .option('transaction','false')\n",
    "         .queryName('winQuery')\n",
    "         .start()\n",
    "    \n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now use a loop to poll the query results from last cell. Generate 20 interations, and pause for 0.5 seconds for each iteration. \n",
    "+ Ensure to exclude 'type' type and only show the total amount if it's no less than 500,000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 190:========================================>           (155 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 195:============================================>       (171 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 197:============================================>       (172 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 205:==============================================>     (179 + 21) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 207:===============================================>    (183 + 17) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 210:===========================================>        (166 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 212:===========================================>        (166 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 220:===========================================>        (168 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 222:===========================================>        (168 + 24) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n",
      "+----+------+-----------+\n",
      "|type|window|TotalAmount|\n",
      "+----+------+-----------+\n",
      "+----+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for x in range(20):\n",
    "    df = spark.sql('SELECT * FROM winQuery WHERE NOT type = \"type\" AND \"amount\" >= 500000000')\n",
    "    df.show(10)\n",
    "    time.sleep(.5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 44.0 in stage 235.0 (TID 15220, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119473,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119474,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119470,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:143)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119471,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119472,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 43.0 in stage 235.0 (TID 15219, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119490,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:208)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1000)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:989)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:960)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:131)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:726)\n",
      "\tat org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:251)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:699)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1032)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:329)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:147)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:196)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:417)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:287)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.InterruptedIOException: java.lang.InterruptedException\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1011)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Object.wait(Object.java:328)\n",
      "\tat java.base/java.lang.ProcessImpl.waitFor(ProcessImpl.java:495)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1001)\n",
      "\t... 47 more\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/24 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:126)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119477,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119478,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119483,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119479,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119480,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119481,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN Shell: Interrupted while joining on: Thread[Thread-119482,5,main]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1043)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1003)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:132)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:89)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$7(statefulOperators.scala:319)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:561)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:277)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.$anonfun$doExecute$5(statefulOperators.scala:319)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$1(package.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:92)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/24 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:126)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/25 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:126)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 42.0 in stage 235.0 (TID 15218, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 24.0 in stage 235.0 (TID 15200, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 22.0 in stage 235.0 (TID 15198, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 25.0 in stage 235.0 (TID 15201, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/38 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:126)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/39 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:126)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 38.0 in stage 235.0 (TID 15214, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 39.0 in stage 235.0 (TID 15215, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/32 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:126)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 41.0 in stage 235.0 (TID 15217, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 40.0 in stage 235.0 (TID 15216, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 32.0 in stage 235.0 (TID 15208, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 37.0 in stage 235.0 (TID 15213, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/30 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:126)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 33.0 in stage 235.0 (TID 15209, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/27 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:126)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/26 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:126)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 27.0 in stage 235.0 (TID 15203, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 30.0 in stage 235.0 (TID 15206, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 26.0 in stage 235.0 (TID 15202, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "java.io.FileNotFoundException: File file:/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/35 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:666)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:987)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:656)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:126)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:91)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:137)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 35.0 in stage 235.0 (TID 15211, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 31.0 in stage 235.0 (TID 15207, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "ExitCodeException exitCode=1: chmod: cannot access '/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/28/..12.delta.a4dd1f42-8a45-4fbf-b1ee-b127d43d3d27.TID15204.tmp.crc': No such file or directory\n",
      "\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1008)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:143)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 36.0 in stage 235.0 (TID 15212, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 28.0 in stage 235.0 (TID 15204, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "ExitCodeException exitCode=1: chmod: cannot access '/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/34/.12.delta.16570974-62af-4d6d-9396-25971c1cff2a.TID15210.tmp': No such file or directory\n",
      "\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1008)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:143)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 34.0 in stage 235.0 (TID 15210, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n",
      "22/04/27 13:08:15 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "ExitCodeException exitCode=1: chmod: cannot access '/tmp/temporary-cbdfe015-e616-4b9f-8c9f-05f136b476f2/state/0/29/..12.delta.5f01915a-5d06-4457-9747-47684c3fe04c.TID15205.tmp.crc': No such file or directory\n",
      "\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1008)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:901)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:252)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:232)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:331)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:320)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:351)\n",
      "\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1228)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:360)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:696)\n",
      "\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:692)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:698)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:310)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:133)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:136)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream$lzycompute(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.deltaFileStream(HDFSBackedStateStoreProvider.scala:95)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream$lzycompute(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.compressedStream(HDFSBackedStateStoreProvider.scala:96)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.abort(HDFSBackedStateStoreProvider.scala:150)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2(package.scala:65)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.package$StateStoreOps.$anonfun$mapPartitionsWithStateStore$2$adapted(package.scala:64)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:143)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/04/27 13:08:15 WARN TaskSetManager: Lost task 29.0 in stage 235.0 (TID 15205, 10.4.10.8, executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "winQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7: Creating a Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a transformation to find the top `'step'`s that have the largest amount of `'newbalanceDest'`. Rename the resulting column to `'totalNewBal'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = streaming.select(streaming.step, streaming.newbalanceDest)\n",
    "\n",
    "topNewBal = (streaming.groupBy('step').agg({'newbalanceDest': 'sum'})\n",
    "            .withColumnRenamed('sum(newbalanceDest)', 'totalNewBal')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- totalNewBal: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topNewBal.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register a query and specify the output sink as memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/27 13:08:20 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cbade78c-e2ac-4258-87a0-5c5d24460124. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "newBalQuery = (topNewBal\n",
    "               .writeStream\n",
    "               .outputMode('complete')\n",
    "               .format('memory')\n",
    "               .option('truncate','false')\n",
    "               .queryName('newBalQuery').start()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import visualization libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph the dashboard. \n",
    "+ Refresh the view every 5 seconds\n",
    "+ Show only the top 10 steps\n",
    "\n",
    "*Note*: at this point, it's likely that your graph may not look 'dynamic' at all, as all the data files have been streamed. You can restart the kernel and click run-all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAHrCAYAAADi27gXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAcQklEQVR4nO3deZRtZ1kn4N9LbkgYBc1FCUEvKNJGhAAhgBGQiBAGwQHQMIgtLhpbbBFbgRUH0KW2Ii5dYqsRwiAQRdRupEGIrYAECNxAEhIikwaIpkkwMgSaIcnbf5x9O8Wlqm4Fvl11TuV51jqr9tlnD+93z767fvXtqbo7AACMcYOdLgAAYDcRrgAABhKuAAAGEq4AAAYSrgAABhKuAAAGWrpwVVWnV9VlVXXBFqa9b1W9s6quqqpHHvTZE6rq/dPrCfNVDABwraULV0lelOTkLU774SQ/kuTla0dW1Vcn+aUk90xyQpJfqqpbjisRAGB9SxeuuvtNSa5YO66qvrGq/qaqzqmqf6iq/zBNe3F3n5/kmoMW86AkZ3b3Fd3970nOzNYDGwDAl23PThewRacleXJ3v7+q7pnkvyc5aZPpb5PkI2veXzKNAwCY1dKHq6q6aZJvT/LnVXVg9BGHmm2dcZ7zAwDMbunDVRaHLj/e3cddh3kuSfKda94fk+QN40oCAFjf0p1zdbDu/mSSf66qRyVJLdzlELO9LskDq+qW04nsD5zGAQDMaunCVVWdkeStSe5YVZdU1ROTPDbJE6vqvCQXJnnENO09quqSJI9K8kdVdWGSdPcVSX4lyTum1y9P4wAAZlXdTkUCABhl6XquAABWmXAFADDQUl0teNRRR/W+fft2ugwAgEM655xzPtbdew8ev1That++fdm/f/9OlwEAcEhV9aH1xjssCAAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAw0J6dLgBYTs/7mb/e6RKus6c893t2ugQAPVcAACMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAPNHq6q6rCqeldVvXrudQEA7LQ927COn0pyUZKbb8O6AEhy0a/+3U6XcJ19y6kn7XQJMMSsPVdVdUyShyZ5/pzrAQBYFnMfFvydJD+X5JqZ1wMAsBRmC1dV9bAkl3X3OYeY7klVtb+q9l9++eVzlQMAsC3m7Lk6McnDq+riJH+a5KSqeunBE3X3ad19fHcfv3fv3hnLAQCY32zhqruf2d3HdPe+JD+U5O+6+3FzrQ8AYBlsx9WCAEvnVx/3yJ0u4To79aWv3OkSgC3YlnDV3W9I8obtWBcAwE5yh3YAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgYQrAICBhCsAgIGEKwCAgWYLV1V1ZFW9varOq6oLq+rZc60LAGBZ7Jlx2Z9LclJ3X1lVhyd5c1W9trvfNuM6AQB21Gzhqrs7yZXT28OnV8+1PpbTib934k6XcJ2c9ZNn7XQJAKy4Wc+5qqrDqurcJJclObO7z55zfQAAO23WcNXdV3f3cUmOSXJCVd3p4Gmq6klVtb+q9l9++eVzlgMAMLttuVqwuz+e5A1JTl7ns9O6+/juPn7v3r3bUQ4AwGzmvFpwb1XdYhq+UZIHJPnHudYHALAM5rxa8NZJXlxVh2UR4l7R3a+ecX0AADtuzqsFz09y17mWDwCwjNyhHQBgIOEKAGAg4QoAYCDhCgBgIOEKAGAg4QoAYCDhCgBgIOEKAGAg4QoAYCDhCgBgIOEKAGAg4QoAYCDhCgBgIOEKAGAg4QoAYCDhCgBgIOEKAGAg4QoAYCDhCgBgIOEKAGAg4QoAYCDhCgBgIOEKAGAg4QoAYKA9G31QVU/bbMbu/u3x5QAArLYNw1WSm21bFQAAu8SG4aq7n72dhQAA7Aab9VwlSarqyCRPTPKtSY48ML67f3TGugAAVtJWTmj/kyRfl+RBSd6Y5Jgkn5qzKACAVbWVcPVN3f0LST7d3S9O8tAk3zZvWQAAq2kr4eoL08+PV9WdknxVkn2zVQQAsMIOec5VktOq6pZJfiHJq5LcdBoGAOAghwxX3f38afCNSW4/bzkAAKtt03BVVfdL8u/dfX5VPTrJfZN8IMkfdPfntqNAAIBVstkd2n8/yZ2THFlV783icODfJPn2JKcneey2VAgAsEI267m6f3cfO93n6l+S3Kq7r66qP0py/vaUBwCwWja7WvCzSdLdn03yoe6+enrfufYKQgAA1tis5+pW08Oba81wpvd7Z68MAGAFbRau/jjXPrx57XCSPP9LJwcA4JAPbq6qI6dDgwAAHMJWbiJ6QVV9NMk/JHlTkrO6+xPzlgUAsJoO+fib7v6mJKckeXeShyU5r6rOnbkuAICVdMieq6o6JsmJSe6T5C5JLkzy5pnrAgBYSVs5LPjhJO9I8mvd/eSZ6wEAWGmHPCyY5K5JXpLkMVX11qp6SVU9cea6AABW0lYe3HxeVX0wyQezODT4uCyeMfiCmWsDAFg5Wznnan+SI5K8JYtzre7b3R+auzAAgFW0lXOuHtzdl89eCQDALrCVc65uUFUvqKrXJklVHeucKwCA9W0lXL0oyeuSHD29f1+Sp85UDwDASttKuDqqu1+R5Jok6e6rklw9a1UAACtqK+Hq01X1NUk6SarqXkk8/gYAYB1bOaH9aUleleQbq+qsJHuTPHLWqgAAVtRW7nP1zqq6X5I7Jqkk7+3uL8xeGQDACtowXFXVfTf46N5Vle5+00w1AQCsrM16rn52nXGdxcObj0ly2CwVAQCssA3DVXd/z9r3VfUdSU5NcmmSp8xcFwDAStrK42++K8kvZNFr9WvdfebsVQEArKjNzrl6aBY9VZ9Icmp3n7VtVQEArKjNeq7+OsklSf4tydOr6os+7O6Hz1gXAMBK2ixc3X/bqgAA2CU2O6H9jdtZCADAbrDZOVfvzvTIm/V0951nqQgAYIVtdljwYdtWBQDALrHZYcEPbWchAAC7wQ0ONUFV3auq3lFVV1bV56vq6qr65HYUBwCwag4ZrpI8L8kpSd6f5EZJfizJ781ZFADAqjrkHdqTpLs/UFWHdffVSV5YVW+ZuS4AgJW0lXD1maq6YZJzq+o3s3i24E3mLQsAYDVt5bDg46fpnpLk00lum+T75ywKAGBVbSVcfW93f7a7P9ndz+7up8VtGgAA1rWVcPWEdcb9yOA6AAB2hc3u0H5KksckuV1VvWrNRzfP4mHOAAAcZLMT2t+SxcnrRyV57prxn0py/pxFAQCsqkPdof1DSe5dVV+b5B7TRxd191XbURwAwKrZyh3aH5Xk7UkeleTRSc6uqkfOXRgAwCrayn2ufj7JPbr7siSpqr1J/jbJK+csDABgFW3lasEbHAhWk3/b4nwAANc7W+m5em1VvS7JGdP7H0zymvlKAgBYXVvpgeokf5TkzknukuS0WSsCAFhhW+m5+u7ufnqSvzwwoqqeneTps1UFALCiNruJ6I8n+c9Jbl9Va+9rdbMkZ81dGADAKtqs5+rlSV6b5NeTPGPN+E919xWHWnBV3TbJS5J8XZJrkpzW3b/7FdQKALD0NruJ6CeSfCLJKV/msq9K8jPd/c6qulmSc6rqzO5+z5e5PACApTfbLRW6+9Lufuc0/KkkFyW5zVzrAwBYBttyv6qq2pfkrknO3o71AQDslNnDVVXdNMlfJHlqd39ync+fVFX7q2r/5ZdfPnc5AACzmjVcVdXhWQSrl3X3X643TXef1t3Hd/fxe/funbMcAIDZzRauqqqSvCDJRd3923OtBwBgmczZc3VikscnOamqzp1eD5lxfQAAO24rd2j/snT3m5PUXMsHAFhG23K1IADA9YVwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADCQcAUAMJBwBQAw0GzhqqpOr6rLquqCudYBALBs5uy5elGSk2dcPgDA0pktXHX3m5JcMdfyAQCW0Y6fc1VVT6qq/VW1//LLL9/pcgAAviI7Hq66+7TuPr67j9+7d+9OlwMA8BXZ8XAFALCbCFcAAAPNeSuGM5K8Nckdq+qSqnriXOsCAFgWe+ZacHefMteyAQCWlcOCAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAA+3Z6QIAgC/2ij8/YadLuE4e/ai373QJS0XPFQDAQMIVAMBAwhUAwEDCFQDAQMIVAMBAwhUAwEDCFQDAQMIVAMBAbiIKwMp51rOetdMlXCerVi9fGT1XAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAAwlXAAADCVcAAAMJVwAAA+2Zc+FVdXKS301yWJLnd/d/m3N9q+jDv/xtO13Cdfb1v/junS4BAJbWbD1XVXVYkt9P8uAkxyY5paqOnWt9AADLYM7Dgick+UB3/1N3fz7JnyZ5xIzrAwDYcXMeFrxNko+seX9Jknte14Xc/WdfMqyg7XLOc354p0sAAHZIdfc8C656VJIHdfePTe8fn+SE7v7Jg6Z7UpInTW/vmOS9sxT0pY5K8rFtWtdO0L7Vpn2raze3LdG+Vad9Y31Dd+89eOScPVeXJLntmvfHJPnXgyfq7tOSnDZjHeuqqv3dffx2r3e7aN9q077VtZvblmjfqtO+7THnOVfvSHKHqrpdVd0wyQ8ledWM6wMA2HGz9Vx191VV9ZQkr8viVgynd/eFc60PAGAZzHqfq+5+TZLXzLmOr8C2H4rcZtq32rRvde3mtiXat+q0bxvMdkI7AMD1kcffAAAMtOvCVVXdtqr+vqouqqoLq+qnpvF/VlXnTq+Lq+rcDeY/uareW1UfqKpnbGvxG6iq06vqsqq6YM2451TVP1bV+VX1V1V1i2n8d1fVOVX17unnSRss86ur6syqev/085bb1JyD69jo+3pWVf3Lmu/sIdP4fVX1f9eM/8MNlrss7Tuyqt5eVedN7Xv2NP5R0/trqur4NdPfsKpeOH1/51XVd26w3GVv30bb525p37rb5/TZM6f9x3ur6kEbLHfZ27fu/nLV9i9TLYdV1buq6tXT+432LV8z7YuurKrnbbK8ZWrbxdN3cW5V7Z/GHVdVbzswrqpOmMYfXlUvnqa/qKqeucEyl6l9t6iqV077kouq6t5V9SvTfuXcqnp9VR09TXvCmu/0vKr6vg2WuT3t6+5d9Upy6yR3m4ZvluR9SY49aJrnJvnFdeY9LMkHk9w+yQ2TnHfwvDvUpvsmuVuSC9aMe2CSPdPwbyT5jWn4rkmOnobvlORfNljmbyZ5xjT8jAPzL8v3leRZSf7rOtPvW/vvsMlyl6V9leSm0/DhSc5Ocq8k35LFfd3ekOT4NdP/RJIXTsO3SnJOkhusYPs22j53S/s22j6PnfYbRyS53bQ/OWzV2nfQNP9/f7lq+5dp/U9L8vIkr57eb/Td3STJdyR5cpLnbbK8ZWrbxUmOOmjc65M8eBp+SJI3TMOPSfKn0/CNp3n3LXn7Xpzkx6bhGya5RZKbr/n8vyT5wzVtOrDPuXWSyw6834n27bqeq+6+tLvfOQ1/KslFWdwtPklSVZXk0UnOWGf2pXxkT3e/KckVB417fXdfNb19Wxb3EUt3v6u7D9xP7MIkR1bVEess9hFZbLiZfn7v6Lq34lDf11dgWdrX3X3l9Pbw6dXdfVF3r3fD3GOT/O9p3suSfDzJevdsWfb2rbt9Zpe0b5NZHpHFL7DPdfc/J/lAFvuV9aZb+vYdvL9ctf1LVR2T5KFJnn+oabv709395iSfPcSkS9G2TXSSm0/DX5Vr7y/ZSW5SVXuS3CjJ55N8cp35l6J9VXXzLDoWXpAk3f357v54d6+t+SaZttfu/syafc6R2fj/6ba0b9eFq7Wqal8Wf2mdvWb0fZJ8tLvfv84s6z2yZ8Qv+rn9aJLXrjP+B5K8q7s/t85nX9vdlyaLgJNFL8KOWuf7esrU/Xv6QV23t5u6+d9YVffZYHFL077psMS5WfwldWZ3n73J5OcleURV7amq2yW5e774ZrwHrFL71m6fu6l9622fW92HrEL7ks33l6uwf/mdJD+X5JqDxm+0b9mKZWlbsggQr58O0R540slTkzynqj6S5LeSHDj898okn05yaZIPJ/mt7r4iX2pZ2nf7JJcneeG0v39+Vd0kSarqV6f2PTbJLx6YoaruWVUXJnl3kievCVtrbUv7dm24qqqbJvmLJE89KOmekvV7rZJFF/nBlvpyyqo6NclVSV520PhvzeJwzH/aibquq3W+rz9I8o1JjstiZ/DcadJLk3x9d981U3f/9BfO0uruq7v7uCx6b06oqjttMvnpWfxC3p/FL4a3ZPH9Lq3N2rfO9rlb2rfR9rly+5BDbJ/r7i9XYf9SVQ9Lcll3n3PQRxt9d6voxO6+W5IHJ/mJqrpvkh9P8tPdfdskP52p5yeLHtSrkxydxSHrn6mq2+9AzVu1J4vTYf5g2t9/OovDeOnuU6f2vSzJUw7M0N1nd/e3JrlHkmdW1ZHbX/bCrgxXVXV4Fr+oX9bdf7lm/J4k35/kzzaYdUuP7FkWVfWEJA9L8tieDiBP449J8ldJfri7P7jB7B+tqltP0x84Pr0j1vu+uvuj007/miR/nOnQynS45d+m4XOyOKflm9dZ7NK074Du/ngW51idvMk0V3X3T3f3cd39iCzOMViv12Dp27fe9rlb2rfR9pmt70OWun3JxvvLFdq/nJjk4VV1cRaneJxUVS/d5LvbqmVoW5LkwCHa6RD7X2XRlickOfB7789zbfsek+RvuvsL0/RnZf1D8svSvkuSXLKmJ/WVWYSttV6eRQ/qF+nui7IIY+v9Ibst7dt14Wo6R+AFSS7q7t8+6OMHJPnH7r5kg9lX5pE9VXVykqcneXh3f2bN+Fsk+V9JntndZ22yiFdl8Z8w08//OVOpm9ro+zqw8U++L8kF0/i9VXXYNHz7JHdI8k/rLHpZ2re3rr1S7kaZtsFNpr/xmq7v705yVXe/Z51Jl7p9m2yfu6V9626fWdT9Q1V1xHTY8w5J3r7Oope6fdPHX7K/XKX9S3c/s7uP6e59WezL/667H7fJd7dVO962JKmqm1TVzQ4MZ3ERyQVZhPn7TZOdlGv/ePlwFgGzpunvlfX3RUvRvu7+P0k+UlV3nEZ9V5L3VNUd1kz28ExtmH5v75mGvyGLC4YuXmfR29O+3qGrAOZ6ZXG1Ryc5P8m50+sh02cvyuI47Nrpj07ymjXvH5LFFWsfTHLqTrdnqumMLLqvv5BFmn9iFifKfmRNGw9cMfHzWST2c9e8bjV99vxMV6Yl+ZosTix+//Tzq5fp+0ryJ1kcNz8/i/8Mt56m/4EsTqQ9L8k7k3zPmmUtY/vunORdUzsuyLVXXX3f9F1+LslHk7xuGr8vyXuzOLH/b7N44voqtm+j7XO3tG/d7XP67NQs9h/vzXTV1qq1b/rsRfnS/eVK7V/W1P2dufZqwc2+u4uzuHjoyun/57HL2rYszkk6b3pdmOn3VRb71HOm8Wcnufs0/qZZ9GRdmOQ9SX52mbfNqZbjsjiF4Pwk/yPJLbM4ynHBNO6vk9xmmvbxU9vOzeJ3w/fuZPvcoR0AYKBdd1gQAGAnCVcAAAMJVwAAAwlXAAADCVcAAAMJV8CuUVVPraob73QdwPWbWzEAu8Z0N+7ju/tjO10LcP21Z6cLAPhyTHeZfkUWj5g5LIsbJB6d5O+r6mPdff+qemCSZyc5Iosbe/7H7r5yCmF/luT+0+Ie090f2O42ALuTw4LAqjo5yb929126+05ZPAj6X5PcfwpWR2VxR/EH9OLhtvuzeNj3AZ/s7hOSPG+aF2AI4QpYVe9O8oCq+o2quk93f+Kgz++V5NgkZ1XVuVk8R+wb1nx+xpqf9567WOD6w2FBYCV19/uq6u5ZPIvy16vq9QdNUknO7O5TNlrEBsMAXxE9V8BKqqqjk3ymu1+a5LeS3C3Jp5LcbJrkbUlOrKpvmqa/cVV985pF/OCan2/dnqqB6wM9V8Cq+rYkz6mqa5J8IcmPZ3F477VVdel03tWPJDmjqo6Y5vn5JO+bho+oqrOz+CNzo94tgOvMrRiA6x23bADm5LAgAMBAeq4AAAbScwUAMJBwBQAwkHAFADCQcAUAMJBwBQAwkHAFADDQ/wMu6D2r/kMK3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    time.sleep(5)\n",
    "    df = spark.sql('SELECT * FROM newBalQuery')\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize = (10,8))\n",
    "    sns.barplot(data = df_pd.head(10), x = 'step', y = \"totalNewBal\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
